import pandas as pd
import numpy as np
from itertools import combinations
import statsmodels.formula.api as smf
from statsmodels.stats.anova import anova_lm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from patsy import dmatrix
from statsmodels.stats.outliers_influence import OLSInfluence
from scipy.stats import f
import warnings
warnings.filterwarnings("ignore")

from statsmodels.formula.api import mixedlm
from statsmodels.tools.sm_exceptions import ConvergenceWarning
warnings.simplefilter("ignore", ConvergenceWarning)
import os
import sys
from io import StringIO

def run_mixed_model_doe_with_output(file_path, output_dir, predictors=None, response_vars=None):
    # å¯¹Xã€Yå˜é‡ååšstripå’Œstrè½¬æ¢ï¼Œå»é™¤å‰åç©ºæ ¼ï¼Œç¡®ä¿å’Œå¼ºåˆ¶èµ‹å€¼æ—¶ä¸€è‡´
    if predictors:
        predictors = [str(x).strip() for x in predictors]
    if response_vars:
        response_vars = [str(y).strip() for y in response_vars]
    print(f"[DEBUG] Cleaned predictors: {predictors}")
    print(f"[DEBUG] Cleaned response_vars: {response_vars}")
    """
    åŸºäºåŸå§‹MixedModelDOE_Function_FollowOriginal_20250804.pyçš„Webè¾“å‡ºç‰ˆæœ¬
    ä¸“é—¨ç”¨äºæ•è·æ§åˆ¶å°è¾“å‡ºå¹¶è¿”å›ç»™Webç•Œé¢æ˜¾ç¤º
    
    æ–°å¢åŠŸèƒ½ï¼š
    1. æ•è·æ‰€æœ‰æ§åˆ¶å°è¾“å‡º
    2. è¿”å›æ ¼å¼åŒ–çš„åˆ†æç»“æœæ–‡æœ¬
    3. ä¿å­˜æ§åˆ¶å°è¾“å‡ºåˆ°æ–‡ä»¶
    4. ä¿æŒåŸå§‹åˆ†æé€»è¾‘ä¸å˜
    """
    
    # ğŸ”§ æ•è·æ‰€æœ‰æ§åˆ¶å°è¾“å‡º
    console_output = StringIO()
    original_stdout = sys.stdout
    
    # é‡å®šå‘è¾“å‡ºåˆ°StringIO
    sys.stdout = console_output
    
    # ç”¨æˆ·å¿…é¡»æ˜¾å¼é€‰æ‹© predictors (X) å’Œ response_vars (Y)
    if not predictors or not isinstance(predictors, list) or len(predictors) == 0:
        raise ValueError("å¿…é¡»é€‰æ‹©è‡³å°‘ä¸€ä¸ªé¢„æµ‹å› å­ (X)ã€‚è¯·åœ¨ç•Œé¢ä¸Šé€‰æ‹©Xå˜é‡ï¼")
    if not response_vars or not isinstance(response_vars, list) or len(response_vars) == 0:
        raise ValueError("å¿…é¡»é€‰æ‹©è‡³å°‘ä¸€ä¸ªå“åº”å˜é‡ (Y)ã€‚è¯·åœ¨ç•Œé¢ä¸Šé€‰æ‹©Yå˜é‡ï¼")
    group_keys = predictors.copy()  # åˆ†ç»„é”®åŠ¨æ€è·ŸéšX

    try:
        # === 1. æ•°æ®å¯¼å…¥ ===
        df_raw = pd.read_csv(file_path)
        # åªä¿ç•™æ•°å€¼å‹predictors
        valid_predictors = [col for col in predictors if col in df_raw.columns and pd.api.types.is_numeric_dtype(df_raw[col])]
        if not valid_predictors:
            raise ValueError("æ— æœ‰æ•ˆæ•°å€¼å‹é¢„æµ‹å› å­å¯ç”¨äºå»ºæ¨¡ï¼")
        predictors = valid_predictors

        print("ğŸš€ å¼€å§‹DOEæ··åˆæ¨¡å‹åˆ†æ...")
        print(f"ğŸ“Š æ•°æ®æ–‡ä»¶: {file_path}")
        print(f"ğŸ“ˆ å“åº”å˜é‡: {response_vars}")
        print(f"ğŸ”§ é¢„æµ‹å› å­: {predictors}")
        print(f"ğŸ“ æ•°æ®ç»´åº¦: {df_raw.shape}")

        # === 2. æ ‡å‡†åŒ–ç”¨äº simplified æ¨¡å‹å»ºæ¨¡ ===
        scaler = StandardScaler()
        df = df_raw.copy()
        df[predictors] = scaler.fit_transform(df[predictors])

        print("\nâœ… æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")
        print("ğŸ“ æ ‡å‡†åŒ–åçš„ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   å‡å€¼: {df[predictors].mean().values}")
        print(f"   æ ‡å‡†å·®: {df[predictors].std(ddof=0).values}")
        print(f"   åŸå§‹å‡å€¼ (X_mean): {scaler.mean_}")
        print(f"   åŸå§‹æ ‡å‡†å·® (X_std): {scaler.scale_}")

        # === 6. æ„é€ åŸå§‹ Config é”®å€¼ï¼ˆJMP å¯¹é½ï¼‰===
        # è‡ªåŠ¨å…¼å®¹ group_keys: å•åˆ—ç›´æ¥ç”¨ï¼Œå¤šåˆ—ç»„åˆ
        valid_group_keys = [col for col in group_keys if col in df_raw.columns]
        if not valid_group_keys:
            # fallback: use default
            valid_group_keys = ["dye1", "dye2", "dye3", "Time", "Temp"]
        if len(valid_group_keys) == 1:
            df_raw["Config_combo"] = df_raw[valid_group_keys[0]].astype(str)
        else:
            df_raw["Config_combo"] = df_raw[valid_group_keys].astype(str).agg("_".join, axis=1)
        df["Config_combo"] = df_raw["Config_combo"]

        # === 3. æ„é€  RSM é¡¹ ===
        def create_rsm_terms(terms):
            linear = terms
            square = [f"I({t}**2)" for t in terms]
            inter = [f"{a}:{b}" for a, b in combinations(terms, 2)]
            return linear + square + inter

        rsm_terms = create_rsm_terms(predictors)
        print(f"\nğŸ”§ æ„é€ RSMé¡¹: {len(rsm_terms)}ä¸ªé¡¹")
        print(f"   çº¿æ€§é¡¹: {predictors}")
        print(f"   å¹³æ–¹é¡¹: {[f'I({t}**2)' for t in predictors]}")
        print(f"   äº¤äº’é¡¹: {[f'{a}:{b}' for a, b in combinations(predictors, 2)]}")

        # === 4. å…¨æ¨¡å‹ LogWorth æ‰«æ ===
        print("\nğŸ“Š å¼€å§‹å…¨æ¨¡å‹LogWorthåˆ†æ...")
        effect_summary_all = pd.DataFrame()
        for y in response_vars:
            formula = f"{y} ~ " + " + ".join(rsm_terms)
            model = smf.ols(formula, data=df).fit()
            anova_tbl = anova_lm(model, typ=3).reset_index()
            anova_tbl = anova_tbl.rename(columns={"index": "Factor"})
            anova_tbl = anova_tbl[anova_tbl["Factor"] != "Residual"]
            anova_tbl["LogWorth"] = -np.log10(anova_tbl["PR(>F)"].replace(0, 1e-16))
            temp = anova_tbl[["Factor", "LogWorth"]].copy()
            temp.columns = ["Factor", y]
            effect_summary_all = pd.merge(effect_summary_all, temp, on="Factor", how="outer") if not effect_summary_all.empty else temp

        effect_summary_all = effect_summary_all.fillna(0)
        effect_summary_all["Median_LogWorth"] = effect_summary_all[response_vars].median(axis=1)
        effect_summary_all["Max_LogWorth"] = effect_summary_all[response_vars].max(axis=1)
        effect_summary_all["Appears_Significant"] = (effect_summary_all[response_vars] > 1.3).sum(axis=1)
        effect_summary_all = effect_summary_all.sort_values("Max_LogWorth", ascending=False)

        # === 5. ç­›é€‰ç®€åŒ–å› å­ï¼ˆä¿æŒ hierarchyï¼‰===
        def get_simplified_factors(effect_matrix, threshold=1.3, min_significant=2):
            factors = effect_matrix[
                (effect_matrix["Max_LogWorth"] >= threshold) | 
                (effect_matrix["Appears_Significant"] >= min_significant)
            ]["Factor"].tolist()
            if "Intercept" in factors:
                factors.remove("Intercept")
            hierarchical_terms = set(factors)
            for f in factors:
                if ":" in f:
                    a, b = f.split(":")
                    hierarchical_terms |= {a.strip(), b.strip()}
                if "I(" in f:
                    base = f.split("(")[1].split("**")[0].strip()
                    hierarchical_terms.add(base)
            return sorted(hierarchical_terms)

        simplified_factors = get_simplified_factors(effect_summary_all)

    # ...existing code...

        # === 7. å…±çº¿æ€§æ£€æŸ¥ ===
        try:
            x = dmatrix(" + ".join(simplified_factors), data=df, return_type="dataframe")
            xtx = x.T @ x
            condition_number = np.linalg.cond(xtx.values)
            print(f"\nğŸ“ å…±çº¿æ€§æ£€æŸ¥ - X'Xæ¡ä»¶æ•°: {condition_number:.2f}")
        except Exception as e:
            print(f"\nâŒ è®¾è®¡çŸ©é˜µæ„å»ºé”™è¯¯: {str(e)}")
            condition_number = float('inf')

        # === 8. æ‰“å°è¾“å‡ºï¼šFull Model + Simplified Model LogWorth ===
        print("\n" + "="*80)
        print("ğŸ“Š å…¨æ¨¡å‹æ•ˆåº”æ±‡æ€»è¡¨ (LogWorth)")
        print("="*80)
        print(effect_summary_all.to_string(index=False))

        print(f"\nâœ… å»ºè®®çš„ç®€åŒ–å› å­ (å«å±‚æ¬¡ç»“æ„): {simplified_factors}")
        print(f"ğŸ“ å…±çº¿æ€§æ£€æŸ¥ - X'Xæ¡ä»¶æ•°: {condition_number:.2f}")

        # æ„å»º simplified_logworth_df
        simplified_logworth_df = pd.DataFrame()
        for y in response_vars:
            print(f"\nğŸ” æ„å»ºç®€åŒ–æ¨¡å‹: {y}")
            formula = f"{y} ~ " + " + ".join(simplified_factors)
            model = smf.ols(formula=formula, data=df).fit()
            anova_tbl = anova_lm(model, typ=3).reset_index()
            anova_tbl = anova_tbl.rename(columns={"index": "Factor"})
            anova_tbl = anova_tbl[anova_tbl["Factor"] != "Residual"]
            anova_tbl["LogWorth"] = -np.log10(anova_tbl["PR(>F)"].replace(0, 1e-16))
            temp = anova_tbl[["Factor", "LogWorth"]].copy()
            temp.columns = ["Factor", y]
            simplified_logworth_df = pd.merge(simplified_logworth_df, temp, on="Factor", how="outer") if not simplified_logworth_df.empty else temp

        simplified_logworth_df = simplified_logworth_df.fillna(0)
        simplified_logworth_df["Median_LogWorth"] = simplified_logworth_df[response_vars].median(axis=1)
        simplified_logworth_df["Max_LogWorth"] = simplified_logworth_df[response_vars].max(axis=1)
        simplified_logworth_df["Appears_Significant"] = (simplified_logworth_df[response_vars] > 1.3).sum(axis=1)
        simplified_logworth_df = simplified_logworth_df.sort_values("Max_LogWorth", ascending=False)

        print("\n" + "="*80)
        print("ğŸ“Š ç®€åŒ–æ¨¡å‹æ•ˆåº”æ±‡æ€»è¡¨ (LogWorth)")
        print("="*80)
        print(simplified_logworth_df.to_string(index=False))

        # === Part 2: æ··åˆæ¨¡å‹å»ºæ¨¡ä¸è¯Šæ–­ ===
        print("\n" + "="*80)
        print("ğŸ”§ å¼€å§‹æ··åˆæ•ˆåº”æ¨¡å‹æ‹Ÿåˆ")
        print("="*80)

        models = {}
        param_coded_list = []
        param_uncoded_list = []
        diagnostics_summary = []
        var_records = []
        lof_records = []

        for y in response_vars:
            try:
                print(f"\nğŸ”§ æ‹Ÿåˆæ··åˆæ¨¡å‹: {y}")
                # æ„å»º Mixed Modelï¼ˆå« Config_combo ä¸ºéšæœºç»„å˜é‡ï¼‰
                formula = f"{y} ~ " + " + ".join(simplified_factors)
                model = mixedlm(formula, data=df, groups=df["Config_combo"])
                model_fit = model.fit(reml=True)
                
                # æ–¹å·®ç»„åˆ†ï¼ˆç”¨äºè¯Šæ–­ï¼‰
                group_var = model_fit.cov_re.iloc[0, 0] if model_fit.cov_re.shape[0] > 0 else np.nan
                residual_var = model_fit.scale  # == RMSEÂ²
                
                print(f"ğŸ“Š æ–¹å·®ç»„åˆ† - {y}:")
                print(f"   ç»„é—´æ–¹å·® (Config): {group_var:.4f}")
                print(f"   æ®‹å·®æ–¹å·® (Error): {residual_var:.4f}")
                print(f"   RMSE: {np.sqrt(residual_var):.4f}")

                var_records.append({
                    "Response": y,
                    "Group_Var": group_var,
                    "Residual_Var": residual_var,
                    "RMSE_from_Var": np.sqrt(residual_var)
                })

                models[y] = model_fit

                y_true = df[y]
                y_pred = model_fit.fittedvalues
                resid = y_true - y_pred

                # è¿‘ä¼¼ RÂ²
                ss_total = np.sum((y_true - y_true.mean()) ** 2)
                ss_resid = np.sum((y_true - y_pred) ** 2)
                r_squared = 1 - ss_resid / ss_total

                # Adjusted RÂ² è¿‘ä¼¼
                k = model_fit.k_fe - 1
                n = len(y_true)
                adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)
                rmse = np.sqrt(np.mean(resid ** 2))

                diagnostics_summary.append({
                    "Response": y,
                    "R2_Approximate": r_squared,
                    "Adjusted_R2_Approximate": adj_r_squared,
                    "RMSE": rmse,
                    "Mean_Response": y_true.mean(),
                    "Observations": n
                })

                # è§£æå›ºå®šæ•ˆåº”å‚æ•°è¡¨
                coef_tbl = model_fit.summary().tables[1].copy()
                coef_tbl.columns = ["Coef.", "Std.Err.", "z", "P>|z|", "[0.025", "0.975]"]
                coef_tbl["P>|z|"] = pd.to_numeric(coef_tbl["P>|z|"], errors="coerce").fillna(1.0)
                coef_tbl["Response"] = y
                coef_tbl["Factor"] = coef_tbl.index
                coef_tbl["LogWorth"] = -np.log10(coef_tbl["P>|z|"].replace(0, 1e-16))
                param_coded_list.append(coef_tbl[["Response", "Factor", "Coef.", "P>|z|", "LogWorth"]])

                # å‚æ•°åæ ‡å‡†åŒ–ï¼ˆè§£ç ï¼‰
                X_mean = scaler.mean_
                X_scale = scaler.scale_
                uncoded = []

                for pname in coef_tbl.index:
                    if pname == "Intercept":
                        continue
                    try:
                        coef_coded = float(coef_tbl.loc[pname, "Coef."])
                    except:
                        continue

                    if pname.startswith("I("):
                        var = pname.split("(")[1].split("**")[0].strip()
                        if var not in predictors: continue
                        i = predictors.index(var)
                        beta_uncoded = coef_coded / (X_scale[i] ** 2)

                    elif ":" in pname:
                        var1, var2 = pname.split(":")
                        if var1 not in predictors or var2 not in predictors: continue
                        i1, i2 = predictors.index(var1), predictors.index(var2)
                        beta_uncoded = coef_coded / (X_scale[i1] * X_scale[i2])

                    else:
                        var = pname.strip()
                        if var not in predictors: continue
                        i = predictors.index(var)
                        beta_uncoded = coef_coded / X_scale[i]

                    uncoded.append((pname, beta_uncoded))

                intercept_uncoded = y_true.mean()
                for pname, beta_uncoded in uncoded:
                    if pname.startswith("I(") or ":" in pname: continue
                    var = pname.strip()
                    if var not in predictors: continue
                    i = predictors.index(var)
                    intercept_uncoded -= beta_uncoded * X_mean[i]

                uncoded.insert(0, ("Intercept", intercept_uncoded))
                uncoded_df = pd.DataFrame(uncoded, columns=["Factor", "Estimate"])
                uncoded_df["Response"] = y
                param_uncoded_list.append(uncoded_df)

                # JMP é£æ ¼ LOF åˆ†æ
                df_raw["_fitted"] = y_pred
                group_df = df_raw.groupby("Config_combo").agg(
                    local_avg=(y, "mean"),
                    fitted_val=("_fitted", "mean"),
                    count=("Config_combo", "count")
                ).reset_index()

                ss_lack = (group_df["count"] * (group_df["local_avg"] - group_df["fitted_val"])**2).sum()
                df_lack = len(group_df) - model_fit.df_modelwc - 1
                df_merge = df_raw.merge(group_df[["Config_combo", "local_avg"]], on="Config_combo", how="left")
                ss_pure = ((df_merge[y] - df_merge["local_avg"])**2).sum()
                df_pure = df_merge.shape[0] - len(group_df)

                ms_lack = ss_lack / df_lack if df_lack > 0 else 0
                ms_pure = ss_pure / df_pure if df_pure > 0 else 0
                F_lof = ms_lack / ms_pure if ms_pure > 0 else 0
                
                from scipy.stats import f as f_dist
                p_lof = 1 - f_dist.cdf(F_lof, df_lack, df_pure) if F_lof > 0 else 1.0

                lof_records.append({
                    "Response": y,
                    "DF_LackOfFit": df_lack,
                    "SS_LackOfFit": ss_lack,
                    "MS_LackOfFit": ms_lack,
                    "DF_PureError": df_pure,
                    "SS_PureError": ss_pure,
                    "MS_PureError": ms_pure,
                    "F_Ratio": F_lof,
                    "p_Value": p_lof
                })

            except Exception as e:
                print(f"âŒ æ¨¡å‹æ‹Ÿåˆå¤±è´¥ - {y}: {e}")

        # === è¯Šæ–­æ±‡æ€»è¾“å‡º ===
        print("\n" + "="*80)
        print("ğŸ“‹ JMPé£æ ¼è¯Šæ–­æ±‡æ€»")
        print("="*80)

        for diag, uncoded_df in zip(diagnostics_summary, param_uncoded_list):
            y = diag["Response"]
            print(f"\nâ–¶ å“åº”å˜é‡: {y}")
            print("-" * 60)
            print(f"è¿‘ä¼¼RÂ²               : {diag['R2_Approximate']:.4f}")
            print(f"è°ƒæ•´RÂ² (è¿‘ä¼¼)        : {diag['Adjusted_R2_Approximate']:.4f}")
            print(f"RMSE                 : {diag['RMSE']:.4f}")
            print(f"å“åº”å˜é‡å‡å€¼         : {diag['Mean_Response']:.4f}")
            print(f"è§‚æµ‹æ•°               : {diag['Observations']}")

            # LOF åˆ†æç»“æœ
            lof_row = next((r for r in lof_records if r["Response"] == y), None)
            if lof_row:
                print(f"\nğŸ”¬ JMPé£æ ¼æ‹Ÿåˆç¼ºå¤±æ£€éªŒ:")
                print(f"æ‹Ÿåˆç¼ºå¤±     â€“ DF={lof_row['DF_LackOfFit']}, SS={lof_row['SS_LackOfFit']:.6f}, MS={lof_row['MS_LackOfFit']:.6f}")
                print(f"çº¯è¯¯å·®       â€“ DF={lof_row['DF_PureError']}, SS={lof_row['SS_PureError']:.6f}, MS={lof_row['MS_PureError']:.6f}")
                print(f"æ€»è¯¯å·®       â€“ DF={lof_row['DF_LackOfFit'] + lof_row['DF_PureError']}, SS={(lof_row['SS_LackOfFit'] + lof_row['SS_PureError']):.6f}")
                print(f"F æ¯”å€¼       : {lof_row['F_Ratio']:.4f}")
                print(f"på€¼          : {lof_row['p_Value']:.5f}")
            
            # æœªç¼–ç å›ºå®šæ•ˆåº”è¡¨
            print(f"\nğŸ“„ å›ºå®šæ•ˆåº”ä¼°è®¡ (æœªç¼–ç ):")
            print(f"{'ä¼°è®¡å€¼':>12s}    {'é¡¹ç›®'}")
            for idx, row in uncoded_df.iterrows():
                print(f"{row['Estimate']:12.6f}    {row['Factor']}")

        # === ä¿å­˜ç»“æœæ–‡ä»¶ ===
        print("\n" + "="*80)
        print("ğŸ’¾ ä¿å­˜åˆ†æç»“æœ")
        print("="*80)
        
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜å›ºå®šæˆªè·
        fixed_intercepts = []
        for y in response_vars:
            beta_0 = models[y].fe_params["Intercept"]
            fixed_intercepts.append({"Response": y, "Fixed_Intercept": beta_0})

        fixed_df = pd.DataFrame(fixed_intercepts)
        fixed_df.to_csv(os.path.join(output_dir, "fixed_intercepts.csv"), index=False)

        # ä¿å­˜å„ç§ç»“æœ
        effect_summary_all.to_csv(os.path.join(output_dir, "fullmodel_logworth.csv"), index=False)
        simplified_logworth_df.to_csv(os.path.join(output_dir, "simplified_logworth.csv"), index=False)
        pd.concat(param_coded_list).to_csv(os.path.join(output_dir, "coded_parameters.csv"), index=False)
        pd.concat(param_uncoded_list).to_csv(os.path.join(output_dir, "uncoded_parameters.csv"), index=False)
        
        diagnostics_df = pd.DataFrame(diagnostics_summary)
        diagnostics_df.to_csv(os.path.join(output_dir, "diagnostics_summary.csv"), index=False)
        
        pd.DataFrame(lof_records).to_csv(os.path.join(output_dir, "JMP_style_lof.csv"), index=False)
        
        # æ ‡å‡†åŒ–ä¿¡æ¯
        pd.DataFrame({
            "Variable": predictors,
            "Mean": scaler.mean_,
            "StdDev": scaler.scale_
        }).to_csv(os.path.join(output_dir, "scaler.csv"), index=False)

        # æ¨¡å‹å…¬å¼
        with open(os.path.join(output_dir, "model_formulas.txt"), "w") as f:
            for y in response_vars:
                formula = f"{y} ~ " + " + ".join(simplified_factors)
                f.write(f"{y} formula:\n{formula}\n\n")

        # æ®‹å·®æ•°æ®
        for y in response_vars:
            try:
                model_fit = models[y]
                y_true = df[y]
                y_pred = model_fit.fittedvalues
                resid = y_true - y_pred

                rmse = np.sqrt(np.mean(resid ** 2))
                pseudo_stud_resid = resid / rmse if rmse > 0 else resid

                df_out = pd.DataFrame({
                    "Config_combo": df["Config_combo"],
                    "Actual": y_true,
                    "Predicted": y_pred,
                    "Residual": resid,
                    "Pseudo_Studentized_Residual": pseudo_stud_resid
                })
                df_out.index.name = "ID"

                out_path = os.path.join(output_dir, f"residual_data_{y}_from_MixedModel.csv")
                df_out.to_csv(out_path)

            except Exception as e:
                print(f"âŒ æ®‹å·®è¾“å‡ºå¤±è´¥ [{y}]: {e}")

        # è®¾è®¡æ•°æ®å’Œå…¶ä»–æ–‡ä»¶
        df_raw.to_csv(os.path.join(output_dir, "design_data.csv"), index=False)
        
        df_var = pd.DataFrame(var_records)
        df_var.to_csv(os.path.join(output_dir, "mixed_model_variance_summary.csv"), index=False)

        brief_path = os.path.join(output_dir, "InputDataBrief.csv")
        brief_df = pd.DataFrame({
            "Variable": predictors,
            "Mean (after standardization)": df[predictors].mean().values,
            "StdDev (after standardization)": df[predictors].std(ddof=0).values,
            "Original Mean (X_mean)": scaler.mean_,
            "Original StdDev (X_std)": scaler.scale_
        })
        brief_df.to_csv(brief_path, index=False)

        print(f"\nâœ… æ‰€æœ‰å»ºæ¨¡ç»“æœå·²åŸºäºæ··åˆæ¨¡å‹å¯¼å‡ºä¸ºCSVï¼Œä¿å­˜åœ¨ï¼š{output_dir}")
        
        # æ–‡ä»¶åˆ—è¡¨
        saved_files = [f for f in os.listdir(output_dir) if f.endswith(('.csv', '.txt'))]
        print(f"ğŸ“ å…±ç”Ÿæˆ {len(saved_files)} ä¸ªç»“æœæ–‡ä»¶:")
        for file in sorted(saved_files):
            print(f"   - {file}")

    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        tb = traceback.format_exc()
        print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{tb}")
        # å…³é”®ï¼šå°†å¼‚å¸¸å †æ ˆä¹Ÿå†™å…¥ Render å¹³å°æ—¥å¿—
        try:
            with open("/tmp/last_error.log", "w", encoding="utf-8") as f:
                f.write(tb)
        except Exception:
            pass
    
    finally:
        # æ¢å¤åŸå§‹è¾“å‡ºå¹¶è·å–æ•è·çš„æ–‡æœ¬
        sys.stdout = original_stdout
        captured_output = console_output.getvalue()
        console_output.close()
        
        # ä¿å­˜æ§åˆ¶å°è¾“å‡ºåˆ°æ–‡ä»¶
        if output_dir and os.path.exists(output_dir):
            console_output_path = os.path.join(output_dir, "console_output.txt")
            with open(console_output_path, "w", encoding="utf-8") as f:
                f.write(captured_output)
        
        # è¿”å›æ§åˆ¶å°è¾“å‡ºå†…å®¹
        return captured_output

# ç›´æ¥è¿è¡Œè„šæœ¬æ—¶çš„å…¥å£
if __name__ == "__main__":
    console_output = run_mixed_model_doe_with_output(
        r"C:\Zhanglei_Microsoft_Upgrade_by_20240905\Pytyon_Study_Local\Color_S2\DOEData_20250622.csv",
        r"C:\Zhanglei_Microsoft_Upgrade_by_20240905\Pytyon_Study_Local\Color_S2\DOE_MixedModel_Outputs"
    )
    print("=" * 80)
    print("ğŸ–¥ï¸  VS Codeç»ˆç«¯è¾“å‡º (ä¸Webè¾“å‡ºå®Œå…¨ç›¸åŒ)")
    print("=" * 80)
    print(console_output)
